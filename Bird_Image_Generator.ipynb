{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bird_Image_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TKkzUNkO2r-0",
        "yGHEADyRZALi",
        "jtYVBopUZYMG",
        "rN67W001ZS2P"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR8RHyW7X0Om",
        "colab_type": "text"
      },
      "source": [
        "##Write captin to Input captions\n",
        "\n",
        "#### key words:\n",
        "*   **Colors**\n",
        "*   **wings**\n",
        "*   **head**\n",
        "*   **belly**\n",
        "*   **beak**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ1F5XLaTV2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input_Caption = \"This bird has red wings black belly yellow head short beak\"\n",
        "\n",
        "\n",
        "\n",
        "#with open(\"example_captions.txt\", 'w') as f:\n",
        "   #f.write(Input_Caption)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/')\n",
        "!rm -r Bird-Image-Generator\n",
        "!git clone https://github.com/ammarnasr/Bird-Image-Generator.git\n",
        "os.chdir('/content/Bird-Image-Generator')\n",
        "\n",
        "\n",
        "the_main()\n",
        "display_images(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGHEADyRZALi",
        "colab_type": "text"
      },
      "source": [
        "##Configerations&Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWGjHhhUpNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####Configerations Part###\n",
        "DATA_DIR = \"../data/birds\"\n",
        "EMBEDDING_DIM = 256\n",
        "NET_G = \"netG_epoch_600.pth\"\n",
        "WORDS_NUM = 18\n",
        "RNN_TYPE = 'LSTM'\n",
        "NET_E = \"text_encoder599.pth\"\n",
        "B_DCGAN = False\n",
        "GF_DIM = 32\n",
        "CONDITION_DIM = 100\n",
        "BRANCH_NUM = 3\n",
        "R_NUM = 2\n",
        "Z_DIM = 100\n",
        "CUDA = True\n",
        "FONT_MAX = 50\n",
        "###########\n",
        "\n",
        "\n",
        "# from easydict import EasyDict as edict\n",
        "# __C = edict()\n",
        "# cfg = __C\n",
        "# __C.GAN = edict()\n",
        "# cfg.GAN.Z_DIM = 100\n",
        "\n",
        "\n",
        "\n",
        "###Imports Part###\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "import shutil \n",
        "import numpy as np\n",
        "import pickle\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import Image as Iamge_ipy\n",
        "from IPython.display import display\n",
        "############\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtYVBopUZYMG",
        "colab_type": "text"
      },
      "source": [
        "##Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFMU1QQdXwA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "###utils functions###\n",
        "\n",
        "def display_images(bs=1):\n",
        "  filepath = \"example_captions.txt\"\n",
        "  with open(filepath, \"r\") as f:\n",
        "    sentences = f.read().split('\\n')\n",
        "  for k in range(bs):\n",
        "    filename0 = '/content/Bird-Image-Generator/netG_epoch_600/example_captions/0_s_%s_g0.png' %k\n",
        "    filename1 = '/content/Bird-Image-Generator/netG_epoch_600/example_captions/0_s_%s_g1.png' %k\n",
        "    filename2 = '/content/Bird-Image-Generator/netG_epoch_600/example_captions/0_s_%s_g2.png' %k\n",
        "    x = Iamge_ipy(filename=filename0) \n",
        "    y = Iamge_ipy(filename=filename1) \n",
        "    z = Iamge_ipy(filename=filename2) \n",
        "    printmd('# **'+sentences[k]+'**')\n",
        "    display(x, y , z)\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "\n",
        "def gen_example(data_dic, n_words):\n",
        "    # Build and load the generator\n",
        "    #print (\"10. creating an object of RNN_ENCODER\")\n",
        "    text_encoder = RNN_ENCODER(n_words, nhidden=EMBEDDING_DIM)\n",
        "    #print(\"15. text_encoder object of RNN_ENCODER created\")\n",
        "    state_dict = torch.load(NET_E, map_location=lambda storage, loc: storage)\n",
        "    text_encoder.load_state_dict(state_dict)\n",
        "    #print(\"16. Loaded weights for text_encoder from:\", NET_E)\n",
        "    #cudaaa text_encoder = text_encoder.cuda()\n",
        "    text_encoder.eval()\n",
        "    #print(\"17. text_encoder moved to gpu and in evaluation mode\")\n",
        "\n",
        "    # the path to save generated images\n",
        "    if B_DCGAN:\n",
        "        netG = G_DCGAN()\n",
        "    else:\n",
        "        #print(\"18. creating an object of G_NET (Generator)\")\n",
        "        netG = G_NET()\n",
        "        #print(\"47. object of G_NET (netG) 'Three stage Generator created'\")\n",
        "    s_tmp = NET_G[:NET_G.rfind('.pth')]\n",
        "    model_dir = NET_G\n",
        "\n",
        "    state_dict = torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
        "    netG.load_state_dict(state_dict)\n",
        "    #print(\"48. Loaded weights for netG from:\", model_dir)\n",
        "    #cudaaa netG.cuda()\n",
        "    netG.eval()\n",
        "    #print(\"49. text_encoder moved to gpu and in evaluation mode\")\n",
        "    for key in data_dic:\n",
        "        save_dir = '%s/%s' % (s_tmp, key)\n",
        "        mkdir_p(save_dir)\n",
        "        #print (\"50. Created directory with name : \", save_dir)\n",
        "        captions, cap_lens, sorted_indices = data_dic[key]\n",
        "        \n",
        "        #print(\"51. fetched captions, cap_lens, sorted_indices from the dictionary\")\n",
        "\n",
        "        batch_size = captions.shape[0]\n",
        "        #print(\"52. got batch size from caption array (number of sentences in file) as :\" ,batch_size)\n",
        "        nz = Z_DIM\n",
        "        captions = Variable(torch.from_numpy(captions), volatile=True)\n",
        "        cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
        "\n",
        "        #cudaaa captions = captions.cuda()\n",
        "        #cudaaa cap_lens = cap_lens.cuda()\n",
        "        #print(\"53. Loaded captions to pytorch and moved to cuda\")\n",
        "        for i in range(1):  # 16\n",
        "            noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
        "            #cudaaa noise = noise.cuda()\n",
        "            #print(\"54. Loaded noise to pytorch and moved to cuda\")\n",
        "            #######################################################\n",
        "            # (1) Extract text embeddings\n",
        "            ######################################################\n",
        "            #print(\"55. Calling init_hidden of RNN_ENCODER (text_encoder)\")\n",
        "            hidden = text_encoder.init_hidden(batch_size)\n",
        "            #print(\"56. finished init_hidden of RNN_ENCODER (text_encoder)\")\n",
        "            # words_embs: batch_size x nef x seq_len\n",
        "            # sent_emb: batch_size x nef\n",
        "            #print(\"57. Calling forward of RNN_ENCODER (text_encoder)\")\n",
        "            words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "            #print(\"58. finished forward of RNN_ENCODER (text_encoder)\")\n",
        "            mask = (captions == 0)\n",
        "            #print(\"59. something to make mask of captions\")\n",
        "            #######################################################\n",
        "            # (2) Generate fake images\n",
        "            ######################################################\n",
        "            noise.data.normal_(0, 1)\n",
        "            #print (\"60. Calling forward of G_NET (netG)\")\n",
        "            fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
        "            #print (\"86. finished forward of G_NET (netG)\")\n",
        "            # G attention\n",
        "            cap_lens_np = cap_lens.cpu().data.numpy()\n",
        "            for j in range(batch_size):\n",
        "                save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n",
        "                #print(\"87.Saving each image generated of the 16 captions with name: (\", save_name,\")\")\n",
        "                for k in range(len(fake_imgs)):\n",
        "                    #print(\"88. looping through the 3 images from each stage to save (\",k+1,\"of 3)\")\n",
        "                    im = fake_imgs[k][j].data.cpu().numpy()\n",
        "                    im = (im + 1.0) * 127.5\n",
        "                    im = im.astype(np.uint8)\n",
        "                    #print('89.image number :', k+1 ,\" with shape: \", im.shape, \"moved to cpu as numpy array\")\n",
        "                    im = np.transpose(im, (1, 2, 0))\n",
        "                    #print('90. image transposed to change to PIL image with shape : ', im.shape)\n",
        "                    im = Image.fromarray(im)\n",
        "                    fullpath = '%s_g%d.png' % (save_name, k)\n",
        "                    im.save(fullpath)\n",
        "                    #print(\"91. image saved to path: \", fullpath)\n",
        "\n",
        "                # for k in range(len(attention_maps)):\n",
        "                #     if len(fake_imgs) > 1:\n",
        "                #         im = fake_imgs[k + 1].detach().cpu()\n",
        "                #     else:\n",
        "                #         im = fake_imgs[0].detach().cpu()\n",
        "                #     attn_maps = attention_maps[k]\n",
        "                #     att_sze = attn_maps.size(2)\n",
        "                #     img_set, sentences = \\\n",
        "                #         build_super_images2(im[j].unsqueeze(0),\n",
        "                #                             captions[j].unsqueeze(0),\n",
        "                #                             [cap_lens_np[j]], wordtoix,\n",
        "                #                             [attn_maps[j]], att_sze)\n",
        "                #     if img_set is not None:\n",
        "                #         im = Image.fromarray(img_set)\n",
        "                #         fullpath = '%s_a%d.png' % (save_name, k)\n",
        "                #         im.save(fullpath)\n",
        "\n",
        "def mkdir_p(path):\n",
        "    #print(\"using mkdir_p\")\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "\n",
        "def upBlock(in_planes, out_planes):\n",
        "    #print(\"using upBlock\")\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv3x3(in_planes, out_planes * 2),\n",
        "        nn.BatchNorm2d(out_planes * 2),\n",
        "        GLU())\n",
        "    return block\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes):\n",
        "    \"1x1 convolution with padding\"\n",
        "    #print(\"using conv1x1\")\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                    padding=0, bias=False)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes):\n",
        "    \"3x3 convolution with padding\"\n",
        "    #print(\"using conv3x3\")\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def describe_tensor (name , tensor):\n",
        "    #print (name ,\": size : \" , tensor.size())\n",
        "    #print (name ,\": first element : \" , tensor[0])\n",
        "    #print (name ,\": non-zeros : \",tensor.view(-1).nonzero().size())\n",
        "    name = \"\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN67W001ZS2P",
        "colab_type": "text"
      },
      "source": [
        "##Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJoVE97wZRKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "###Class of Models###\n",
        "class RNN_ENCODER(nn.Module):\n",
        "    def __init__(self,\n",
        "                ntoken,\n",
        "                ninput=300,\n",
        "                drop_prob=0.5,\n",
        "                nhidden=128,\n",
        "                nlayers=1,\n",
        "                bidirectional=True):\n",
        "        super(RNN_ENCODER, self).__init__()\n",
        "        self.n_steps = WORDS_NUM\n",
        "        self.ntoken = ntoken  # size of the dictionary that maps words to unique indexes\n",
        "        self.ninput = ninput  # size of each embedding vector\n",
        "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
        "        self.nlayers = nlayers  # Number of recurrent layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = RNN_TYPE\n",
        "        if bidirectional:\n",
        "            self.num_directions = 2\n",
        "        else:\n",
        "            self.num_directions = 1\n",
        "        # number of features in the hidden state\n",
        "        self.nhidden = nhidden // self.num_directions\n",
        "\n",
        "        #print (\"11. calling define_module of RNN_ENCODER\")\n",
        "        self.define_module()\n",
        "        #print (\"13. calling init_weights of RNN_ENCODER\")\n",
        "        self.init_weights()\n",
        "\n",
        "    def define_module(self):\n",
        "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # dropout: If non-zero, introduces a dropout layer on\n",
        "            # the outputs of each RNN layer except the last layer\n",
        "            self.rnn = nn.LSTM(self.ninput,\n",
        "                                self.nhidden,\n",
        "                                self.nlayers,\n",
        "                                batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "            #print (\"12.finshed define_module of RNN_ENCODER\")\n",
        "        elif self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(self.ninput,\n",
        "                                self.nhidden,\n",
        "                                self.nlayers,\n",
        "                                batch_first=True,\n",
        "                                dropout=self.drop_prob,\n",
        "                                bidirectional=self.bidirectional)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # Do not need to initialize RNN parameters, which have been initialized\n",
        "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
        "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        # self.decoder.bias.data.fill_(0)\n",
        "        #print (\"14. finished init_weights of RNN_ENCODER\")\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (Variable( weight.new(self.nlayers * self.num_directions, bsz, self.nhidden).zero_()),\n",
        "                    Variable( weight.new(self.nlayers * self.num_directions, bsz, self.nhidden).zero_()))\n",
        "        else:\n",
        "            return Variable(\n",
        "                weight.new(self.nlayers * self.num_directions, bsz,\n",
        "                           self.nhidden).zero_())\n",
        "\n",
        "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
        "        # input: torch.LongTensor of size batch x n_steps\n",
        "        # --> emb: batch x n_steps x ninput\n",
        "        emb = self.drop(self.encoder(captions))\n",
        "        #print(\"--------------------------------------\")\n",
        "        describe_tensor(\"emb\" , emb)\n",
        "        #\n",
        "        # Returns: a PackedSequence object\n",
        "        cap_lens = cap_lens.data.tolist()\n",
        "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
        "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
        "        # tensor containing the initial hidden state for each element in batch.\n",
        "        # #output (batch, seq_len, hidden_size * num_directions)\n",
        "        # #or a PackedSequence object:\n",
        "        # tensor containing output features (h_t) from the last layer of RNN\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # PackedSequence object\n",
        "        # --> (batch, seq_len, hidden_size * num_directions)\n",
        "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
        "\n",
        "\n",
        "        \n",
        "        # output = self.drop(output)\n",
        "        # --> batch x hidden_size*num_directions x seq_len\n",
        "        words_emb = output.transpose(1, 2)\n",
        "        # --> batch x num_directions*hidden_size\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
        "        else:\n",
        "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
        "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
        "\n",
        "        return words_emb, sent_emb\n",
        "\n",
        "class CA_NET(nn.Module):\n",
        "    # some code is modified from vae examples\n",
        "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
        "    def __init__(self):\n",
        "        super(CA_NET, self).__init__()\n",
        "        self.t_dim = EMBEDDING_DIM\n",
        "        self.c_dim = CONDITION_DIM\n",
        "        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n",
        "        #print(\"20. Creating an object of GLU\")\n",
        "        self.relu = GLU()\n",
        "        #print(\"21. object of GLU (relu) created\")\n",
        "\n",
        "    def encode(self, text_embedding):\n",
        "        x = self.relu(self.fc(text_embedding))\n",
        "        mu = x[:, :self.c_dim]\n",
        "        logvar = x[:, self.c_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        if CUDA:\n",
        "            #cudaaa eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "            eps = torch.FloatTensor(std.size()).normal_()\n",
        "        else:\n",
        "            eps = torch.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        #print(\"62. Calling encode of CA_NET\")\n",
        "        mu, logvar = self.encode(text_embedding)\n",
        "        #print(\"63. finished encode of CA_NET\")\n",
        "        #print(\"64. Calling reparamatrize of CA_NET\")\n",
        "        c_code = self.reparametrize(mu, logvar)\n",
        "        #print(\"65. finished reparamatrize of CA_NET\")\n",
        "        describe_tensor(\"text_embedding\",text_embedding)\n",
        "        describe_tensor(\"mu\",mu)\n",
        "        describe_tensor(\"logvar\",logvar)\n",
        "        describe_tensor(\"c_code\",c_code)\n",
        "        return c_code, mu, logvar\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GLU, self).__init__()\n",
        "        #print(\"Using GLU\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        nc = x.size(1)\n",
        "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
        "        nc = int(nc / 2)\n",
        "        return x[:, :nc] * F.sigmoid(x[:, nc:])\n",
        "\n",
        "class INIT_STAGE_G(nn.Module):\n",
        "    def __init__(self, ngf, ncf):\n",
        "        super(INIT_STAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.in_dim = Z_DIM + ncf  # cfg.TEXT.EMBEDDING_DIM\n",
        "        #print(\"24. calling define module of INIT_STAGE_G \")\n",
        "        self.define_module()\n",
        "\n",
        "    def define_module(self):\n",
        "        nz, ngf = self.in_dim, self.gf_dim\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n",
        "            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n",
        "            GLU())\n",
        "        #print(\"25.Creating 4 upBlock Layers (upBlock: Sequential of: nn.Upsample, conv3x3, nn.BatchNorm2d, GLU)\")\n",
        "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
        "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
        "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
        "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
        "        #print(\"26.upsample1, upsample2, upsample3 and upsample4 careted \")\n",
        "        #print(\"27. finished define module of INIT_STAGE_G\")\n",
        "\n",
        "    def forward(self, z_code, c_code):\n",
        "        \"\"\"\n",
        "        :param z_code: batch x cfg.GAN.Z_DIM\n",
        "        :param c_code: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "        :return: batch x ngf/16 x 64 x 64\n",
        "        \"\"\"\n",
        "        c_z_code = torch.cat((c_code, z_code), 1)\n",
        "        # state size ngf x 4 x 4\n",
        "        out_code = self.fc(c_z_code)\n",
        "        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n",
        "        # state size ngf/3 x 8 x 8\n",
        "        out_code = self.upsample1(out_code)\n",
        "        # state size ngf/4 x 16 x 16\n",
        "        out_code = self.upsample2(out_code)\n",
        "        # state size ngf/8 x 32 x 32\n",
        "        out_code32 = self.upsample3(out_code)\n",
        "        # state size ngf/16 x 64 x 64\n",
        "        out_code64 = self.upsample4(out_code32)\n",
        "\n",
        "        return out_code64\n",
        "\n",
        "class GET_IMAGE_G(nn.Module):\n",
        "    def __init__(self, ngf):\n",
        "        super(GET_IMAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.img = nn.Sequential(\n",
        "            conv3x3(ngf, 3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, h_code):\n",
        "        out_img = self.img(h_code)\n",
        "        return out_img\n",
        "\n",
        "class GlobalAttentionGeneral(nn.Module):\n",
        "    def __init__(self, idf, cdf):\n",
        "        super(GlobalAttentionGeneral, self).__init__()\n",
        "        #print(\"34. make conv_context of conv1x1\")\n",
        "        self.conv_context = conv1x1(cdf, idf)\n",
        "        self.sm = nn.Softmax()\n",
        "        self.mask = None\n",
        "\n",
        "    def applyMask(self, mask):\n",
        "        self.mask = mask  # batch x sourceL\n",
        "\n",
        "    def forward(self, input, context):\n",
        "        \"\"\"\n",
        "            input: batch x idf x ih x iw (queryL=ihxiw)\n",
        "            context: batch x cdf x sourceL\n",
        "        \"\"\"\n",
        "        ih, iw = input.size(2), input.size(3)\n",
        "        queryL = ih * iw\n",
        "        batch_size, sourceL = context.size(0), context.size(2)\n",
        "\n",
        "        # --> batch x queryL x idf\n",
        "        target = input.view(batch_size, -1, queryL)\n",
        "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
        "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
        "        sourceT = context.unsqueeze(3)\n",
        "        # --> batch x idf x sourceL\n",
        "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
        "\n",
        "        # Get attention\n",
        "        # (batch x queryL x idf)(batch x idf x sourceL)\n",
        "        # -->batch x queryL x sourceL\n",
        "        attn = torch.bmm(targetT, sourceT)\n",
        "        # --> batch*queryL x sourceL\n",
        "        attn = attn.view(batch_size*queryL, sourceL)\n",
        "        if self.mask is not None:\n",
        "            # batch_size x sourceL --> batch_size*queryL x sourceL\n",
        "            mask = self.mask.repeat(queryL, 1)\n",
        "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
        "        attn = self.sm(attn)  # Eq. (2)\n",
        "        # --> batch x queryL x sourceL\n",
        "        attn = attn.view(batch_size, queryL, sourceL)\n",
        "        # --> batch x sourceL x queryL\n",
        "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
        "\n",
        "        # (batch x idf x sourceL)(batch x sourceL x queryL)\n",
        "        # --> batch x idf x queryL\n",
        "        weightedContext = torch.bmm(sourceT, attn)\n",
        "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
        "        attn = attn.view(batch_size, -1, ih, iw)\n",
        "\n",
        "        return weightedContext, attn\n",
        "\n",
        "class NEXT_STAGE_G(nn.Module):\n",
        "    def __init__(self, ngf, nef, ncf):\n",
        "        super(NEXT_STAGE_G, self).__init__()\n",
        "        self.gf_dim = ngf\n",
        "        self.ef_dim = nef\n",
        "        self.cf_dim = ncf\n",
        "        self.num_residual = R_NUM\n",
        "        #print(\"32. calling define_module of NEXT_STAGE_G \")\n",
        "        self.define_module()\n",
        "        #print(\"39. finished define_module of NEXT_STAGE_G\")\n",
        "\n",
        "    def _make_layer(self, block, channel_num):\n",
        "        layers = []\n",
        "        for i in range(R_NUM):\n",
        "            layers.append(block(channel_num))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def define_module(self):\n",
        "        ngf = self.gf_dim\n",
        "        #print(\"33.Creating an object of GlobalAttentionGeneral\")\n",
        "        self.att = GlobalAttentionGeneral(ngf, self.ef_dim)\n",
        "        #print(\"35. object of GlobalAttentionGeneral (att) created\")\n",
        "        #print(\"36. calling _make_layer of NEXT_STAGE_G (R_NUM=2 of resBlocks) \")\n",
        "        self.residual = self._make_layer(ResBlock, ngf * 2)\n",
        "        #print(\"37. finished _make_layer of NEXT_STAGE_G\")\n",
        "        #print(\"38. Creating 1 upBlock Layer\")\n",
        "        self.upsample = upBlock(ngf * 2, ngf)\n",
        "\n",
        "    def forward(self, h_code, c_code, word_embs, mask):\n",
        "        \"\"\"\n",
        "            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n",
        "            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n",
        "            c_code1: batch x idf x queryL\n",
        "            att1: batch x sourceL x queryL\n",
        "        \"\"\"\n",
        "        #print (\"73. Calling applyMask of GlobalAttentionGeneral(att)\")\n",
        "        self.att.applyMask(mask)\n",
        "        #print (\"74. finished applyMask of GlobalAttentionGeneral(att)\")\n",
        "        #print (\"75. Calling forward of GlobalAttentionGeneral(att)\")\n",
        "        c_code, att = self.att(h_code, word_embs)\n",
        "        #print (\"76. finished forward of GlobalAttentionGeneral(att)\")\n",
        "        h_c_code = torch.cat((h_code, c_code), 1)\n",
        "        out_code = self.residual(h_c_code)\n",
        "\n",
        "        # state size ngf/2 x 2in_size x 2in_size\n",
        "        out_code = self.upsample(out_code)\n",
        "\n",
        "        return out_code, att\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channel_num):\n",
        "        super(ResBlock, self).__init__()\n",
        "        #print(\"using ResBlock(Sequential : conv3x3, nn.BatchNorm2d, GLU, conv3x3, nn.BatchNorm2d)\")\n",
        "        self.block = nn.Sequential(\n",
        "            conv3x3(channel_num, channel_num * 2),\n",
        "            nn.BatchNorm2d(channel_num * 2),\n",
        "            GLU(),\n",
        "            conv3x3(channel_num, channel_num),\n",
        "            nn.BatchNorm2d(channel_num))\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.block(x)\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "class G_NET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(G_NET, self).__init__()\n",
        "        ngf = GF_DIM\n",
        "        nef = EMBEDDING_DIM\n",
        "        ncf = CONDITION_DIM\n",
        "        #print(\"19.Creating an object CA_NET\")\n",
        "        self.ca_net = CA_NET()\n",
        "        #print(\"22.object of CA_NET (ca_net) created\")\n",
        "\n",
        "        if BRANCH_NUM > 0:\n",
        "            #print(\"23.Creating an object INIT_STAGE_G (first Generator)\")\n",
        "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
        "            #print(\"28. object of INIT_STAGE_G (h_net1) created\")\n",
        "            #print(\"29. Creating an object of GET_IMAGE_G\")\n",
        "            self.img_net1 = GET_IMAGE_G(ngf)\n",
        "            #print(\"30. object of GET_IMAGE_G (img_net1) created\")\n",
        "        # gf x 64 x 64\n",
        "        if BRANCH_NUM > 1:\n",
        "            #print(\"31.Creating an object NEXT_STAGE_G (second Generator)\")\n",
        "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "            #print(\"40.object of NEXT_STAGE_G(h_net2) created\")\n",
        "            #print(\"41. Creating an object of GET_IMAGE_G\")\n",
        "            self.img_net2 = GET_IMAGE_G(ngf)\n",
        "            #print(\"42. object of GET_IMAGE_G (img_net2) created\")\n",
        "        if BRANCH_NUM > 2:\n",
        "            #print(\"43.Creating an object NEXT_STAGE_G (third Generator)\")\n",
        "            #print(\"--------------BeginDuplicate--------------\")\n",
        "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
        "            #print(\"---------------EndDuplicate---------------\")\n",
        "            #print(\"44.object of NEXT_STAGE_G(h_net3) created\")\n",
        "            #print(\"45. Creating an object of GET_IMAGE_G\")\n",
        "            self.img_net3 = GET_IMAGE_G(ngf)\n",
        "            #print(\"46. object of GET_IMAGE_G (img_net3) created\")\n",
        "\n",
        "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
        "        \"\"\"\n",
        "            :param z_code: batch x cfg.GAN.Z_DIM\n",
        "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
        "            :param word_embs: batch x cdf x seq_len\n",
        "            :param mask: batch x seq_len\n",
        "            :return:\n",
        "        \"\"\"\n",
        "        describe_tensor(\"z_code \" , z_code)\n",
        "        describe_tensor(\" sent_emb\" ,sent_emb )\n",
        "        describe_tensor(\"word_embs\" ,word_embs )\n",
        "        describe_tensor(\"mask \" ,mask )\n",
        "        fake_imgs = []\n",
        "        att_maps = []\n",
        "        #print (\"61. Calling forward of CA_NET (ca_net)\")\n",
        "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
        "        #print (\"66. finished forward of CA_NET (ca_net)\")\n",
        "\n",
        "        if BRANCH_NUM > 0:\n",
        "            #print (\"67. Calling forward of INIT_STAGE_G (h_net1)\")\n",
        "            h_code1 = self.h_net1(z_code, c_code)\n",
        "            #print (\"68. finished forward of INIT_STAGE_G (h_net1)\")\n",
        "            #print (\"69. Calling forward of GET_IMAGE_G (img_net1)\")\n",
        "            fake_img1 = self.img_net1(h_code1)\n",
        "            #print (\"70. finished forward of GET_IMAGE_G (img_net1)\")\n",
        "            fake_imgs.append(fake_img1)\n",
        "            #print (\"71. appended first stage images (fake_imgs1) to the full list (fake_imgs) \")\n",
        "        if BRANCH_NUM > 1:\n",
        "            #print (\"72. Calling forward of NEXT_STAGE_G (h_net2)\")\n",
        "            h_code2, att1 = self.h_net2(h_code1, c_code, word_embs, mask)\n",
        "            #print (\"77. finshed forward of NEXT_STAGE_G (h_net2)\")\n",
        "            #print (\"78. Calling forward of GET_IMAGE_G (img_net2)\")\n",
        "            fake_img2 = self.img_net2(h_code2)\n",
        "            #print (\"79. finished forward of GET_IMAGE_G (img_net2)\")\n",
        "            fake_imgs.append(fake_img2)\n",
        "            #print (\"80. appended second stage images (fake_imgs2) to the full list (fake_imgs) \")\n",
        "            if att1 is not None:\n",
        "                att_maps.append(att1)\n",
        "        if BRANCH_NUM > 2:\n",
        "            #print (\"81. Calling forward of NEXT_STAGE_G (h_net3)\")\n",
        "            #print(\"--------------BeginDuplicate--------------\")\n",
        "            h_code3, att2 = self.h_net3(h_code2, c_code, word_embs, mask)\n",
        "            #print(\"---------------EndDuplicate---------------\")\n",
        "            #print (\"82. finshed forward of NEXT_STAGE_G (h_net3)\")\n",
        "            #print (\"83. Calling forward of GET_IMAGE_G (img_net3)\")\n",
        "            fake_img3 = self.img_net3(h_code3)\n",
        "            #print (\"84. Calling forward of GET_IMAGE_G (img_net3)\")\n",
        "            fake_imgs.append(fake_img3)\n",
        "            #print (\"85. appended third stage images (fake_imgs3) to the full list (fake_imgs) \")\n",
        "            if att2 is not None:\n",
        "                att_maps.append(att2)\n",
        "\n",
        "        return fake_imgs, att_maps, mu, logvar\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFyMG43kZt4B",
        "colab_type": "text"
      },
      "source": [
        "##Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpgFNE-SZsPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def the_main():\n",
        "\n",
        "  ###Generate Examples###\n",
        "  #os.chdir('/content/Bird-Image-Generator/')\n",
        "  print(os.getcwd())\n",
        "  shutil.rmtree(\"/content/Bird-Image-Generator/netG_epoch_600\")\n",
        "  #print(\"0.This where Execution Start\")\n",
        "  filepath = 'captions.pickle'  # Load captions.pickle contain four parts :\n",
        "  #captions[0] : train_captions, 88550 caption each of different length of words (9-25),each word represented by unique index\n",
        "  #captions[1] : test_captions , 29330 caption each of different length of words (9-25),each word represented by unique index\n",
        "  #captions[2] : index to word dictionary that maps 5450 indexes[key] to words[value]\n",
        "  #captions[3] : word to index dictionary that maps 5450 words[key] to indexes[value]\n",
        "\n",
        "  with open(filepath, 'rb') as f:\n",
        "      x = pickle.load(f)\n",
        "      ixtoword, wordtoix = x[2], x[3]\n",
        "      del x\n",
        "      n_words = len(wordtoix)\n",
        "      #print(\"1.Opened and Loaded captions.pickle to fetch wordtoix of length : \", n_words)\n",
        "      filepath = 'example_filenames.txt' #a group of example filenames, 24 names for 24 file, each file with a number of captions\n",
        "\n",
        "      data_dic = {}  # dictionary used to generate images from captions\n",
        "      #key : name of file from which we got captions\n",
        "      #value: [padded-sorted_based_on_length-indexed captions, original length of each(before padding, indexes to order based on length)\n",
        "\n",
        "  with open(filepath, \"r\") as f:\n",
        "      filenames = f.read().split('\\n')\n",
        "      #print(\"2.Opened and loaded example_filename.txt\")\n",
        "      first_counter = 0\n",
        "      for name in filenames:\n",
        "          first_counter += 1\n",
        "          if name == \"example_captions\":  #Keep this way until you download the rest of the file captions\n",
        "              filepath = '%s.txt' % (name)\n",
        "              with open(filepath, \"r\") as f:\n",
        "                  sentences = f.read().split('\\n')\n",
        "                  #print(\"3.Opened and loaded example_captions.txt(\",first_counter,\") in loop of all names of filename\")\n",
        "                  # split your text file of 16 captions to a list of 16 string entries\n",
        "                  captions = []\n",
        "                  cap_lens = []\n",
        "                  second_counter = 0\n",
        "                  for sent in sentences:\n",
        "                      second_counter += 1\n",
        "                      sent = sent.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                      tokenizer = RegexpTokenizer(r'\\w+')\n",
        "                      tokens = tokenizer.tokenize(sent.lower())\n",
        "                      #convert a single sentence(string) to list of tokens(words)=>result in a list of string entries that are word that make up the original sentence(caption)\n",
        "                      if second_counter == 0 :\n",
        "                        second_counter = 0\n",
        "                        #print(\"4.Tokenized a sentence from example file in filenames (\", second_counter,\")\" )\n",
        "                      rev = []\n",
        "                      third_counter = 0\n",
        "                      for t in tokens:\n",
        "                          third_counter += 1\n",
        "                          t = t.encode('ascii', 'ignore').decode('ascii')\n",
        "                          rev.append(wordtoix[t])\n",
        "                          if third_counter == 0 :\n",
        "                            third_counter = 0\n",
        "                            #print (\"5.append converted token to new list(\",third_counter,\")\")\n",
        "                              #convert the list of words to a list crosspending indexes\n",
        "                      captions.append(rev)  # all captions in the file\n",
        "                      cap_lens.append(len(rev))\n",
        "                      if second_counter == 0:\n",
        "                        second_counter = 0\n",
        "                        #print(\"6.append converted sentence to list of captions (\",second_counter,\")\")\n",
        "                      # the length(number of words/tokens) in each caption\n",
        "              max_len = np.max(cap_lens)  # used to pad shorter captions\n",
        "              sorted_indices = np.argsort(cap_lens)[::-1] # Returns the indices that would sort the array of lengths.\n",
        "              cap_lens = np.asarray(cap_lens)\n",
        "              cap_lens = cap_lens[sorted_indices]# sort the array of lengths using the sotring indices\n",
        "              cap_array = np.zeros((len(captions), max_len), dtype='int64')  #placeholder for the padded sorted array caption\n",
        "              for i in range(len(captions)):\n",
        "                  idx = sorted_indices[i]\n",
        "                  cap = captions[idx]\n",
        "                  c_len = len(cap)\n",
        "                  cap_array[i, :c_len] = cap\n",
        "              #print(\"7.created padded sorted array of caption cap_array (\", first_counter,\")\")\n",
        "              key = name[(name.rfind('/') + 1):]\n",
        "              data_dic[key] = [cap_array, cap_lens, sorted_indices]\n",
        "              #print(\"8. created data_dic of cap_array , cap_lens & sorted_indices (\", first_counter,\")\")\n",
        "\n",
        "\n",
        "  #print(\"9. Calling gen_example(data_dic) \")\n",
        "  gen_example(data_dic ,n_words )\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ny_XZdxUxp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}